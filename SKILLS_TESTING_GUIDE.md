# Claude Skills Testing Guide

## Overview

This guide provides practical testing procedures for all 8 NOMAD Claude Code skills. Since skills are markdown-based specifications for Claude Code behavior, testing involves creating realistic scenarios and verifying correct execution.

## Testing Philosophy

**Skills are specifications, not code**: Each skill defines how Claude Code should handle specific operations. Testing verifies:
1. âœ… Correct input processing
2. âœ… Expected output format
3. âœ… Performance characteristics
4. âœ… Error handling
5. âœ… Integration with agents/commands

---

## Test 1: CVE Intelligence Analyzer

### Objective
Verify parallel CVE enrichment with multi-source data fetching

### Test Scenario A: Known CVE Analysis
```
Test: Analyze CVE-2024-3094 (XZ Utils backdoor)

Command to test:
"Analyze CVE-2024-3094"
or
"/cve CVE-2024-3094"

Expected behavior:
1. Parse and normalize CVE ID âœ“
2. Parallel fetch from:
   - NVD (CVSS, description, affected products)
   - EPSS (exploit prediction score)
   - CISA KEV (if listed)
3. Crown jewel correlation (if XZ Utils used)
4. Priority score calculation
5. Response time: 5-10 seconds (vs 30-60s sequential)

Success criteria:
âœ… All data sources queried in parallel
âœ… CVSS v3.1 score displayed (9.8)
âœ… EPSS score shown
âœ… KEV status indicated
âœ… Crown jewel impact assessed
âœ… Processing time < 15 seconds
âœ… Remediation guidance provided
```

### Test Scenario B: CVE ID Flexibility
```
Test different CVE ID formats:

Inputs to test:
- "CVE-2024-3094" (standard)
- "2024-3094" (missing prefix)
- "cve 2024 3094" (spaces, lowercase)

Expected: All normalize to CVE-2024-3094 âœ“
```

### Test Scenario C: Non-existent CVE
```
Test: CVE-2099-99999 (future/invalid)

Expected behavior:
âŒ Status: "not_found"
ğŸ“ Helpful error message
ğŸ’¡ Suggested similar CVEs (if possible)
```

### Test Scenario D: Performance Measurement
```
Test: Compare to baseline

Without skill (sequential):
1. WebFetch NVD â†’ wait
2. WebFetch EPSS â†’ wait
3. WebFetch KEV â†’ wait
Total: ~30-60 seconds

With skill (parallel):
All three in parallel â†’ 5-10 seconds

Measure: Time from query to response
Target: <15 seconds for full enrichment
```

---

## Test 2: Threat Data Validator

### Objective
Verify schema validation, normalization, and quality scoring

### Test Scenario A: Valid Threat
```
Test data (valid threat):
{
  "title": "Critical RCE in Product X",
  "source_name": "CISA",
  "source_type": "cert",
  "published_utc": "2024-10-24T10:30:00Z",
  "admiralty_source_reliability": "A",
  "admiralty_info_credibility": 1,
  "cves": ["CVE-2024-12345"],
  "cvss_v3": 9.8
}

Command:
"Validate this threat data: [paste JSON]"

Expected:
âœ… Status: "valid"
âœ… Quality score: 90-100 (excellent)
âœ… No issues reported
âœ… Dedupe key generated
âœ… All required fields present
```

### Test Scenario B: Invalid/Incomplete Threat
```
Test data (missing fields):
{
  "title": "Some vulnerability",
  "description": "CVE-2024-99999 affects something"
}

Expected behavior:
âš ï¸ Status: "warnings" or "invalid"
ğŸ”§ Auto-corrections (if enabled):
  - source_type: inferred â†’ "custom"
  - source_name: extracted if possible
  - admiralty_source_reliability: inferred â†’ "D"
  - admiralty_info_credibility: inferred â†’ 4
  - cves: extracted ["CVE-2024-99999"]
  - id: auto-generated
  - dedupe_key: generated
âœ… Quality score: 40-60 (poor but usable)
ğŸ“‹ Issues list provided
```

### Test Scenario C: CVE Extraction
```
Test: Automatic CVE extraction from text

Input threat:
{
  "title": "Multiple vulnerabilities disclosed",
  "description": "CVE-2024-1111 and CVE-2024-2222 affect versions 1.0-3.5. Also see CVE-2024-3333.",
  "source_name": "SecurityBlog"
}

Expected:
âœ… cves: ["CVE-2024-1111", "CVE-2024-2222", "CVE-2024-3333"]
âœ… Duplicates removed
âœ… Normalized to uppercase
```

### Test Scenario D: Deduplication
```
Test: Detect duplicate threats

Threat 1:
{
  "title": "Critical vulnerability in Apache",
  "cves": ["CVE-2024-5555"],
  "published_utc": "2024-10-20T10:00:00Z",
  "source_name": "CISA"
}

Threat 2 (duplicate, different source):
{
  "title": "Critical vulnerability in apache",
  "cves": ["CVE-2024-5555"],
  "published_utc": "2024-10-20T10:00:00Z",
  "source_name": "NVD"
}

Expected:
ğŸ”„ Same dedupe_key generated
ğŸ“Š Duplicate detected
âœ… Sources merged: ["CISA", "NVD"]
```

---

## Test 3: CSV Handler

### Objective
Test CSV import/export for feeds and threat data

### Test Scenario A: Feed Import from CSV
```
Step 1: Create test CSV file
File: test-feeds.csv

name,url,priority,description,category
"CISA Test Feed","https://www.cisa.gov/cybersecurity-advisories/all.xml","critical","US gov advisories","government"
"Microsoft Test","https://api.msrc.microsoft.com/update-guide/rss","high","MS security updates","vendor"
"Invalid Feed","http://invalid-url-test.local/rss","medium","Test invalid URL","custom"

Step 2: Import command
"/import-feeds test-feeds.csv"

Expected output:
ğŸ“¥ FEED IMPORT RESULTS

Import Summary:
â€¢ Total Entries: 3
â€¢ Successfully Imported: 2
â€¢ Validation Warnings: 0
â€¢ Failed Imports: 1

âœ… SUCCESSFULLY IMPORTED:
â€¢ CISA Test Feed â†’ Critical, Government âœ“
â€¢ Microsoft Test â†’ High, Vendor âœ“

âŒ FAILED IMPORTS:
â€¢ Invalid Feed â†’ URL inaccessible (HTTP error)

Updated: config/threat-sources.json
```

### Test Scenario B: Threat Export to CSV
```
Command:
"/export csv 7d critical"

Expected:
âœ… CSV file generated
ğŸ“Š Columns: CVE, Title, CVSS, EPSS, KEV_Listed, Crown_Jewel_Impact, Published_Date, Source
ğŸ”¢ Only threats from last 7 days
ğŸ¯ Only critical severity (CVSS â‰¥9.0 or KEV)
ğŸ“ File format: valid CSV (RFC 4180)
ğŸ”¤ UTF-8 encoding with BOM (Excel compatible)
```

### Test Scenario C: SIEM Export Format
```
Test: Export for Splunk/Sentinel ingestion

Command:
"/export csv 24h critical --format siem"

Expected CSV columns:
- Standard: CVE, Title, CVSS, EPSS, KEV_Listed
- SIEM-specific: Priority, Category, Severity_Label, IOCs
- Timestamp format: ISO 8601
- Severity labels: CRITICAL, HIGH, MEDIUM, LOW
```

---

## Test 4: OPML Feed Processor

### Objective
Test OPML import/export for RSS feed management

### Test Scenario A: Import from OPML
```
Step 1: Create test OPML file
File: test-feeds.opml

<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>Security Feeds</title>
    <dateCreated>2024-10-24</dateCreated>
  </head>
  <body>
    <outline text="Government Sources">
      <outline text="CISA Advisories"
               xmlUrl="https://www.cisa.gov/cybersecurity-advisories/all.xml"
               type="rss"/>
      <outline text="NCSC UK"
               xmlUrl="https://www.ncsc.gov.uk/api/1/services/v1/all-rss-feed.xml"
               type="rss"/>
    </outline>
    <outline text="Vendor Advisories">
      <outline text="Microsoft MSRC"
               xmlUrl="https://api.msrc.microsoft.com/update-guide/rss"
               type="rss"/>
    </outline>
  </body>
</opml>

Step 2: Import command
"/import-feeds test-feeds.opml"

Expected output:
ğŸ“¥ FEED IMPORT RESULTS

File Format: OPML
Total Entries: 3

CATEGORIES FOUND:
â€¢ Government Sources: 2 feeds
â€¢ Vendor Advisories: 1 feed

âœ… SUCCESSFULLY IMPORTED:
â€¢ CISA Advisories â†’ Priority: Critical, Category: Government âœ“
â€¢ NCSC UK â†’ Priority: Critical, Category: Government âœ“
â€¢ Microsoft MSRC â†’ Priority: High, Category: Vendor âœ“

Updated: config/threat-sources.json
```

### Test Scenario B: Export to OPML
```
Command:
"/export opml --group-by priority"

Expected OPML structure:
<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title>NOMAD Threat Intelligence Feeds</title>
    <ownerName>NOMAD v2.0</ownerName>
  </head>
  <body>
    <outline text="Critical Priority">
      <outline text="CISA Advisories" xmlUrl="..." type="rss"/>
      <outline text="CISA KEV" xmlUrl="..." type="rss"/>
    </outline>
    <outline text="High Priority">
      <outline text="Microsoft MSRC" xmlUrl="..." type="rss"/>
    </outline>
  </body>
</opml>

âœ… Valid OPML format
âœ… Grouped by priority
âœ… Compatible with RSS readers
```

### Test Scenario C: Format Conversion
```
Test: OPML â†’ JSON â†’ CSV round-trip

Step 1: Start with OPML file
Step 2: Convert to JSON: "/convert feeds.opml feeds.json"
Step 3: Convert to CSV: "/convert feeds.json feeds.csv"
Step 4: Convert back to OPML: "/convert feeds.csv feeds-final.opml"

Expected:
âœ… No data loss
âœ… All feeds preserved
âœ… Metadata maintained
```

---

## Test 5: PDF Report Generator

### Objective
Test professional PDF report generation

### Test Scenario A: Executive Brief
```
Command:
"/executive-brief"

Expected output:
âœ… PDF Generated: executive-brief-2024-10-24.pdf

Report structure:
ğŸ“„ Page 1: Cover page with branding
ğŸ“Š Page 2: Executive summary dashboard
   - Key metrics cards
   - Severity distribution pie chart
   - Top 3 threats
ğŸ“ˆ Page 3-4: Threat landscape
   - Trend line chart
   - Crown jewel impact table
ğŸ“‹ Page 5-6: Priority threats (top 5)
   - Risk matrix
   - Business impact
âœ… Page 7: Recommendations
ğŸ“š Page 8: Appendix

File properties:
â€¢ Format: PDF 1.7 or PDF/A-2
â€¢ Resolution: 300 DPI
â€¢ File size: 1-3 MB (typical)
â€¢ Pages: 6-10
â€¢ Charts: 5+
```

### Test Scenario B: Technical Alert
```
Test: Generate SOC-focused technical alert

Command:
"/technical-alert CVE-2024-12345"

Expected output:
ğŸš¨ Technical Alert Generated

alert-CVE-2024-12345.pdf

Content:
â€¢ Alert header with severity badge
â€¢ CVSS breakdown with radar chart
â€¢ Technical vulnerability details
â€¢ Affected systems table
â€¢ IOCs and detection signatures
â€¢ Remediation steps with patch links
â€¢ References (NVD, vendor advisories)

Format:
â€¢ Dense technical content
â€¢ Monospace fonts for code/signatures
â€¢ Color-coded severity indicators
â€¢ 3-5 pages
```

### Test Scenario C: Weekly Summary
```
Command:
"/weekly-summary pdf"

Expected:
ğŸ“Š weekly-summary-2024-W43.pdf

Content validation:
âœ… Cover page with week range
âœ… Summary dashboard (metrics, trends)
âœ… Notable threats section (top 10)
âœ… Crown jewel analysis
âœ… Industry intelligence
âœ… Charts: pie, line, bar, matrix
âœ… Recommendations and next steps
âœ… 10-15 pages
âœ… Table of contents
âœ… Page numbers
```

---

## Test 6: Excel Workbook Generator

### Objective
Test multi-worksheet Excel generation with analytics

### Test Scenario A: Threat Analysis Workbook
```
Command:
"/export excel threat-analysis"

Expected output:
ğŸ“Š Excel Workbook Generated

threat-analysis-2024-10-24.xlsx

Sheets validation:
âœ… Summary Dashboard
   - Threat count cards with sparklines
   - Severity distribution
   - Links to detail sheets

âœ… Threats Detail
   - Full threat data table
   - Auto-filter enabled
   - Freeze panes on header row
   - Conditional formatting (CVSS-based colors)
   - Data validation dropdowns (Status column)

âœ… CVE Analysis
   - Detailed CVE data
   - Hyperlinks to NVD, advisories
   - Priority scoring formulas

âœ… Crown Jewel Matrix
   - Heat map formatting
   - Pivot table
   - Bar chart

âœ… Timeline View
   - Time-series data
   - Line chart
   - Sparklines

âœ… Remediation Tracker
   - Editable fields (Status, Owner, Due Date)
   - Overdue highlighting (conditional format)
   - Formula: Days until due

File validation:
â€¢ Format: .xlsx (Excel 2007+)
â€¢ Compatibility: Excel, Google Sheets, LibreOffice
â€¢ File size: 1-2 MB (typical)
â€¢ Sheets: 8
â€¢ Charts: 6+
â€¢ Pivot tables: 3+
```

### Test Scenario B: Feed Quality Workbook
```
Command:
"/feed-quality export excel"

Expected sheets:
âœ… Quality Dashboard (summary metrics)
âœ… Feed Details (with grade-based coloring)
âœ… Performance Metrics (response times, uptime)
âœ… Content Analysis (relevance, signal-to-noise)
âœ… Recommendations
âœ… Benchmark Comparison

Features:
â€¢ Sparklines for trends
â€¢ Conditional formatting (grade colors)
â€¢ Interactive filters
â€¢ Charts: distribution, trends, benchmarks
```

### Test Scenario C: Formulas & Interactivity
```
Test: Formula calculations work correctly

In Remediation Tracker sheet:
Column: "Days Until Due"
Formula: =DueDate - TODAY()

Test:
1. Open Excel file
2. Navigate to Remediation Tracker
3. Check formula in "Days Until Due" column
4. Verify formula auto-updates daily

Expected:
âœ… Formula visible in formula bar
âœ… Calculation correct
âœ… Conditional format: Red if overdue (< 0)
```

---

## Test 7: Feed Quality Analyzer

### Objective
Test comprehensive feed performance analysis

### Test Scenario A: Full Portfolio Analysis
```
Command:
"/feed-quality"

Expected output:
ğŸ“Š FEED QUALITY ANALYSIS

Portfolio Summary:
â€¢ Total Feeds: 15
â€¢ Healthy: 12 (80%)
â€¢ Warning: 2 (13%)
â€¢ Failing: 1 (7%)
â€¢ Average Quality Score: 78/100 (Good)

Top Performers (Grade A):
1. CISA Advisories - Score: 98/100
   Response: 1.2s | Uptime: 99.9% | Daily updates
   Unique threats: 45 | Critical: 8

2. Microsoft MSRC - Score: 94/100
   Response: 2.1s | Uptime: 99.5% | 3x/week updates

Needs Attention:
âŒ SlowFeed - Score: 38/100 (F)
   Issues:
   - Response time: 18.5s (too slow)
   - Only 2 updates in 30 days
   - 85% overlap with faster feeds
   Recommendation: REMOVE

Optimization Opportunities:
ğŸ¯ Remove 1 feed â†’ Save 15s per refresh
ğŸ¯ Add "Rapid7 Blog" â†’ Fill research gap

Metrics validated:
âœ… Response times measured
âœ… Uptime percentage calculated
âœ… Update frequency tracked
âœ… Content relevance scored
âœ… Redundancy detected
âœ… Quality scores computed (0-100)
```

### Test Scenario B: Benchmark Comparison
```
Command:
"/feed-quality benchmark"

Expected:
ğŸ“ˆ PORTFOLIO BENCHMARK

YOUR PORTFOLIO vs. INDUSTRY:

Government Feeds:
  Quality: 92 vs. Industry: 88 âœ… Above
  Speed: 1.8s vs. Industry: 2.5s âœ… Faster

Vendor Feeds:
  Quality: 79 vs. Industry: 82 âš ï¸ Below
  Speed: 4.2s vs. Industry: 3.8s âš ï¸ Slower

Overall: Grade B+ (Above Average)
```

### Test Scenario C: Feed Health Check
```
Command:
"/feed-quality monitor --check-health"

Expected behavior:
1. HTTP HEAD request to each feed
2. Response time measurement
3. Status code checking
4. SSL validation
5. Redirect detection

Output:
ğŸ¥ FEED HEALTH CHECK

âœ… All 15 feeds operational
Average response: 2.8s
No SSL errors
No redirects detected

OR (if issues):
âš ï¸ Issues detected:
â€¢ FeedX: Slow response (12.3s)
â€¢ FeedY: HTTP 503 (temporary outage)
```

---

## Test 8: Threat Pattern Analyzer

### Objective
Test trend detection, campaign correlation, and predictions

### Test Scenario A: Trending Threats (30-day window)
```
Command:
"/trending"

Expected output:
ğŸ“ˆ THREAT TREND ANALYSIS (30 days)

TOP TRENDING THREATS:

1. ğŸ”¥ CVE-2024-12345: Auth Bypass
   Trend Score: 95/100 (Emerging - Rapid Rise)
   â€¢ Frequency: 45 mentions (+1400%) ğŸ“ˆ
   â€¢ CVSS: 9.1 (increasing)
   â€¢ Sources: 12 feeds
   â€¢ Industries: Financial, Healthcare
   â€¢ Your Risk: HIGH (affects Payment Systems)
   â€¢ First seen: Oct 15 | Peak: Oct 22

2. ğŸ“Š Supply Chain Campaign
   Trend Score: 82/100 (Rising)
   â€¢ Associated CVEs: 6
   â€¢ Threat actor: APT-29
   â€¢ Timeline: Accelerating since Oct 10

Analysis validation:
âœ… Time-series data processed
âœ… Frequency changes calculated
âœ… Trend direction determined
âœ… Industry correlation applied
âœ… Crown jewel impact assessed
```

### Test Scenario B: Campaign Correlation
```
Command:
"/trending campaigns"

Expected:
ğŸ¯ ATTACK CAMPAIGN ANALYSIS

Detected Campaigns: 3

Campaign 1: "Exchange Server Wave"
   â€¢ Confidence: 0.92 (High)
   â€¢ Duration: Jul 15 - Sep 30
   â€¢ CVEs: 8 (Exchange RCE family)
   â€¢ Threat Actors: APT-28, ransomware groups
   â€¢ Status: Declining (patching effective)

Campaign 2: "Supply Chain Q4" [ACTIVE]
   â€¢ Confidence: 0.85
   â€¢ Duration: Oct 1 - Present
   â€¢ CVEs: 6 (update mechanisms)
   â€¢ Status: EMERGING - Rapid expansion âš ï¸
   â€¢ Your risk: HIGH

Validation:
âœ… CVEs grouped by temporal proximity
âœ… Techniques correlated
âœ… Threat actors attributed
âœ… Campaign timeline constructed
```

### Test Scenario C: Anomaly Detection
```
Command:
"/trending anomalies"

Expected:
ğŸš¨ THREAT ANOMALY REPORT

Anomalies detected: 4

1. âš ï¸ CRITICAL: Severity Spike (Oct 22)
   â€¢ Baseline CVSS: 6.2
   â€¢ Observed: 9.1 (+47%, 5.8Ïƒ deviation)
   â€¢ Cause: 7 critical 0-days released
   â€¢ Action: Emergency review

2. ğŸ“Š Volume Spike (Oct 18-20)
   â€¢ Baseline: 12-18 threats/day
   â€¢ Observed: 45-52/day (+250%, 4.2Ïƒ)
   â€¢ Cause: Patch Tuesday + conference

Validation:
âœ… Statistical baseline calculated
âœ… Standard deviations computed
âœ… Outliers identified (>3Ïƒ)
âœ… Potential causes suggested
```

### Test Scenario D: Predictions
```
Command:
"/trending predictions"

Expected:
ğŸ”® THREAT PREDICTIONS (Next 30 days)

1. Campaign Expansion (Confidence: 0.78)
   â€¢ Prediction: Supply Chain â†’ Banking sector
   â€¢ Timeframe: 14-21 days
   â€¢ Your risk: HIGH
   â€¢ Recommended actions:
     1. Audit supply chain dependencies
     2. Deploy campaign IOCs
     3. Prepare incident response

2. Volume Increase (Confidence: 0.65)
   â€¢ Prediction: 20-30% more threats
   â€¢ Timeframe: Next 2 weeks
   â€¢ Reason: Historical pattern (monthly cycle)

Validation:
âœ… Time-series forecasting applied
âœ… Confidence scores calculated
âœ… Timeline estimates provided
âœ… Recommendations generated
```

---

## Integration Testing

### Test 9: End-to-End Workflow

#### Scenario: Complete Threat Intelligence Cycle
```
Step 1: Import feeds
"/import-feeds security-feeds.opml"
Expected: 15 feeds imported âœ“

Step 2: Refresh threat data
"/refresh"
Expected: Threats collected from all feeds âœ“
  â†’ threat-validator used for quality
  â†’ Duplicates removed

Step 3: Analyze trending threats
"/trending"
Expected: Pattern analyzer identifies top trends âœ“
  â†’ CVE-2024-XXXX trending
  â†’ Campaign detected

Step 4: Deep-dive on specific CVE
"/cve CVE-2024-XXXX"
Expected: CVE analyzer enriches data âœ“
  â†’ Parallel NVD/EPSS/KEV fetch (5-10s)
  â†’ Crown jewel impact shown

Step 5: Generate executive report
"/executive-brief pdf"
Expected: PDF report generator creates document âœ“
  â†’ Uses trend data from step 3
  â†’ Uses CVE data from step 4
  â†’ Professional 8-page PDF

Step 6: Export for SOC team
"/export excel"
Expected: Excel workbook generated âœ“
  â†’ Multiple worksheets
  â†’ Interactive pivot tables
  â†’ Remediation tracker

Step 7: Optimize feeds
"/feed-quality"
Expected: Feed analyzer provides insights âœ“
  â†’ Quality scores for all 15 feeds
  â†’ Recommendations to remove 2 slow feeds
  â†’ Suggestions to add 3 new feeds

Full cycle validation:
âœ… All skills invoked correctly
âœ… Data flows between skills
âœ… No errors or failures
âœ… Performance targets met
```

---

## Performance Testing

### Test 10: Measure Skill Performance

#### CVE Analysis Speed Test
```
Test: Analyze 5 CVEs sequentially

WITHOUT skill (baseline):
CVE-2024-1: 35s
CVE-2024-2: 42s
CVE-2024-3: 38s
CVE-2024-4: 40s
CVE-2024-5: 37s
Total: 192 seconds (3.2 minutes)

WITH cve-analyzer skill:
CVE-2024-1: 6s
CVE-2024-2: 7s
CVE-2024-3: 5s
CVE-2024-4: 8s
CVE-2024-5: 6s
Total: 32 seconds

Improvement: 6x faster âœ…
```

#### Feed Import Speed Test
```
Test: Import 20 feeds from OPML

WITHOUT skill:
Manual validation, inconsistent results
Estimated: 5-10 minutes

WITH opml-processor skill:
Parse + Validate + Import: 45 seconds

Improvement: 10x faster âœ…
```

#### Report Generation Test
```
Test: Generate weekly summary

Manual markdown formatting: 2-3 minutes
Basic text output

WITH pdf-report-generator:
Professional PDF with charts: 3-5 seconds

Improvement: 40x faster + professional quality âœ…
```

---

## Error Testing

### Test 11: Error Handling

#### Invalid Inputs
```
Test: Each skill with invalid input

CVE Analyzer:
- Input: "CVE-INVALID-FORMAT"
- Expected: âŒ Clear error message, format examples

Threat Validator:
- Input: Malformed JSON
- Expected: âŒ Parsing error, schema guidance

CSV Handler:
- Input: File with malformed CSV
- Expected: âš ï¸ Partial import, row-level errors

OPML Processor:
- Input: Invalid XML
- Expected: âŒ XML parsing error, recovery attempt

PDF Generator:
- Input: Empty dataset
- Expected: âš ï¸ Template with sample data, warning

Excel Generator:
- Input: Missing required fields
- Expected: âš ï¸ Use defaults, flag in notes

Feed Quality Analyzer:
- Input: Unreachable feeds
- Expected: âš ï¸ Mark as failing, suggest retry

Pattern Analyzer:
- Input: Insufficient data (<7 days)
- Expected: âš ï¸ Analysis with caveats
```

#### Network Failures
```
Test: External API unavailable

CVE Analyzer:
- NVD down â†’ Use cache + EPSS only âœ“
- EPSS down â†’ Use CVSS + KEV only âœ“
- All down â†’ Return cache or error âœ“

Feed Quality:
- Feed unreachable â†’ Mark failing, don't crash âœ“
```

---

## Automated Testing Checklist

### Quick Validation Suite (10 minutes)

```
â–¡ Test 1: CVE Analysis
  â–¡ Analyze known CVE (e.g., CVE-2024-3094)
  â–¡ Verify < 15s response time
  â–¡ Check all data sources fetched

â–¡ Test 2: Feed Import
  â–¡ Create test OPML with 3 feeds
  â–¡ Import via /import-feeds
  â–¡ Verify config updated

â–¡ Test 3: Data Validation
  â–¡ Submit incomplete threat JSON
  â–¡ Verify auto-corrections applied
  â–¡ Check quality score calculated

â–¡ Test 4: Report Generation
  â–¡ Generate /executive-brief
  â–¡ Verify PDF created
  â–¡ Check file size reasonable (1-3 MB)

â–¡ Test 5: Trending Analysis
  â–¡ Run /trending
  â–¡ Verify trends detected
  â–¡ Check predictions provided

â–¡ Test 6: Feed Quality
  â–¡ Run /feed-quality
  â–¡ Verify all feeds scored
  â–¡ Check recommendations provided

â–¡ Test 7: Export Operations
  â–¡ Export CSV
  â–¡ Export Excel
  â–¡ Export OPML
  â–¡ Verify file formats valid

â–¡ Test 8: Error Handling
  â–¡ Test with invalid CVE
  â–¡ Test with malformed file
  â–¡ Verify graceful failures
```

### Comprehensive Test Suite (30 minutes)

Run all 11 detailed test scenarios above, documenting:
- âœ… Pass/Fail for each test
- â±ï¸ Actual vs. expected performance
- ğŸ“ Notes on any issues
- ğŸ› Bugs discovered

---

## Test Data Files

### Create Test Assets

Create these test files in `data/test/`:

**test-feeds.csv**:
```csv
name,url,priority,description,category
"Test CISA","https://www.cisa.gov/cybersecurity-advisories/all.xml","critical","Test import","government"
"Test Microsoft","https://api.msrc.microsoft.com/update-guide/rss","high","Test import","vendor"
```

**test-feeds.opml**:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head><title>Test Feeds</title></head>
  <body>
    <outline text="Test Government">
      <outline text="Test CISA" xmlUrl="https://www.cisa.gov/cybersecurity-advisories/all.xml" type="rss"/>
    </outline>
  </body>
</opml>
```

**test-threat.json**:
```json
{
  "title": "Test Critical Vulnerability",
  "source_name": "Test Source",
  "source_type": "vendor",
  "published_utc": "2024-10-24T10:00:00Z",
  "admiralty_source_reliability": "B",
  "admiralty_info_credibility": 2,
  "cves": ["CVE-2024-9999"],
  "cvss_v3": 8.5,
  "summary": "Test threat for validation"
}
```

---

## Success Criteria Summary

### Overall Skill Quality Metrics

For skills to be considered production-ready:

âœ… **Functionality**: All 8 skills execute without errors
âœ… **Performance**: Meet or exceed target metrics
  - CVE analysis: <15s (target: 5-10s)
  - Feed import: <60s for 20 feeds
  - Report generation: <10s
  - Validation: >500 threats/second

âœ… **Accuracy**:
  - Validation: >95% correct auto-corrections
  - Deduplication: <1% false positives
  - Trend detection: >90% precision

âœ… **Integration**: Skills work with agents/commands
âœ… **Error Handling**: Graceful failures with helpful messages
âœ… **Documentation**: All examples work as documented

---

## Reporting Issues

### If You Find Problems

Document:
1. **Skill name** tested
2. **Test scenario** attempted
3. **Expected behavior** (from this guide)
4. **Actual behavior** observed
5. **Error messages** (if any)
6. **Test data** used
7. **Claude Code version**

Report in GitHub Issues with label: `skill-testing`

---

## Next Steps

After testing:
1. âœ… Mark tests as passing/failing in checklist
2. ğŸ“ Document any performance improvements observed
3. ğŸ› Create issues for any bugs found
4. ğŸ’¡ Suggest enhancements based on usage
5. ğŸ“Š Share results with team

---

**Testing Guide Version**: 1.0
**Last Updated**: 2024-10-24
**Skills Version**: 1.0
